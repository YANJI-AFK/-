import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from tqdm import tqdm


class GradientBoostingPredictor:
    def __init__(self, model_path='gradient_boosting_deep_trained.pkl'):
        self.model_info = self.load_model(model_path)
        self.model = self.model_info['model']
        self.scaler = self.model_info['scaler']
        self.feature_columns = self.model_info['feature_columns']
        self.target_columns = self.model_info['target_columns']

        print("âœ… å·²åŠ è½½æ¨¡å‹ä¿¡æ¯:")
        print(f"   - æ¨¡å‹ç±»å‹: GradientBoosting")
        print(f"   - ç‰¹å¾æ•°é‡: {len(self.feature_columns)}")
        print(f"   - ç›®æ ‡æ•°é‡: {len(self.target_columns)}")
        print(f"   - è®­ç»ƒæ—¶é—´: {self.model_info.get('train_time', 'æœªçŸ¥')}")

    def load_model(self, model_path):
        """åŠ è½½å®Œæ•´æ¨¡å‹ä¿¡æ¯"""
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½GradientBoostingæ¨¡å‹...")
        with open(model_path, 'rb') as f:
            model_info = pickle.load(f)
        return model_info

    def load_historical_data(self, data_path='ç”µåŠ›æ•°æ®.csv'):
        """åŠ è½½å†å²æ•°æ®ï¼ˆç¡®ä¿ä¸è®­ç»ƒæ—¶æ ¼å¼ä¸€è‡´ï¼‰"""
        print("ğŸ“‚ æ­£åœ¨åŠ è½½å†å²æ•°æ®...")
        data = pd.read_csv(data_path)
        data['date'] = pd.to_datetime(data['date'])
        print(f"âœ… å†å²æ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {data.shape}")
        return data

    def prepare_future_features(self, historical_data, future_dates):
        """å‡†å¤‡æœªæ¥ç‰¹å¾ï¼ˆä¸¥æ ¼éµå¾ªè®­ç»ƒæ—¶çš„ç‰¹å¾å·¥ç¨‹é€»è¾‘ï¼‰"""
        print("ğŸ”„ å‡†å¤‡æœªæ¥ç‰¹å¾...")

        # åˆ›å»ºæœªæ¥æ•°æ®æ¡†æ¶
        future_data = pd.DataFrame({'date': future_dates})

        # æ—¶é—´ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
        future_data['day_of_week'] = future_data['date'].dt.dayofweek
        future_data['day_of_month'] = future_data['date'].dt.day
        future_data['month'] = future_data['date'].dt.month
        future_data['is_weekend'] = future_data['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)
        future_data['is_workday'] = future_data['day_of_week'].apply(lambda x: 1 if x < 5 else 0)

        # å‘¨æœŸæ€§ç‰¹å¾ï¼ˆæ­£å¼¦/ä½™å¼¦ç¼–ç ï¼‰
        future_data['day_of_week_sin'] = np.sin(2 * np.pi * future_data['day_of_week'] / 7)
        future_data['day_of_week_cos'] = np.cos(2 * np.pi * future_data['day_of_week'] / 7)
        future_data['month_sin'] = np.sin(2 * np.pi * future_data['month'] / 12)
        future_data['month_cos'] = np.cos(2 * np.pi * future_data['month'] / 12)

        # å¤©æ°”ç‰¹å¾ï¼ˆè¿™é‡Œç”¨å†å²åŒæœŸå¹³å‡å€¼å¡«å……ï¼Œå®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨å¤©æ°”é¢„æŠ¥æ•°æ®ï¼‰
        # å…³é”®ï¼šç¡®ä¿ç‰¹å¾åç§°å’Œæ•°é‡ä¸è®­ç»ƒæ—¶ä¸€è‡´
        weather_features = ['max_temp', 'min_temp', 'avg_temp', 'temp_diff',
                            'day_wind_level', 'night_wind_level', 'avg_wind_level',
                            'is_rainy', 'is_extreme_weather', 'weather_encoded']

        for feat in weather_features:
            if feat in historical_data.columns:
                # ç”¨å†å²åŒæœŸæ•°æ®çš„ç»Ÿè®¡å€¼å¡«å……
                future_data[feat] = historical_data.groupby(['month', 'day_of_month'])[feat].transform('median').mean()
            else:
                # å¦‚æœè®­ç»ƒæ—¶å­˜åœ¨è¯¥ç‰¹å¾ï¼Œå¡«å……é»˜è®¤å€¼
                future_data[feat] = 0

        # ç¡®ä¿æ‰€æœ‰è®­ç»ƒæ—¶çš„ç‰¹å¾éƒ½å­˜åœ¨
        for col in self.feature_columns:
            if col not in future_data.columns:
                future_data[col] = 0  # ç¼ºå¤±ç‰¹å¾å¡«å……é»˜è®¤å€¼

        # åªä¿ç•™è®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾ï¼Œé¡ºåºä¸¥æ ¼ä¸€è‡´
        future_data = future_data[self.feature_columns]
        print(f"âœ… æœªæ¥ç‰¹å¾å‡†å¤‡å®Œæˆï¼Œå½¢çŠ¶: {future_data.shape}")

        # æ£€æŸ¥ç‰¹å¾å®Œæ•´æ€§
        missing_features = set(self.feature_columns) - set(future_data.columns)
        extra_features = set(future_data.columns) - set(self.feature_columns)

        if missing_features:
            print(f"âš ï¸ ç¼ºå¤±ç‰¹å¾: {missing_features}")
            raise ValueError(f"ç¼ºå¤±è®­ç»ƒæ—¶çš„å…³é”®ç‰¹å¾: {missing_features}")
        if extra_features:
            print(f"âš ï¸ æœ‰ {len(extra_features)} ä¸ªé¢å¤–ç‰¹å¾ï¼Œå°†è¢«å¿½ç•¥")
            future_data = future_data[self.feature_columns]

        print(f"âœ… æœ€ç»ˆç‰¹å¾æ•°é‡: {len(future_data.columns)}")
        return future_data

    def predict_future_3months(self, historical_data):
        """é¢„æµ‹æœªæ¥3ä¸ªæœˆè´Ÿè·ï¼ˆä¿®å¤ç‰¹å¾å¯¹é½é—®é¢˜ï¼‰"""
        print("\nğŸš€ å¼€å§‹é¢„æµ‹æœªæ¥3ä¸ªæœˆè´Ÿè·...")

        # ç”Ÿæˆæœªæ¥3ä¸ªæœˆæ—¥æœŸï¼ˆæ’é™¤é‡å¤æ—¥æœŸï¼‰
        last_date = historical_data['date'].max()
        future_dates = []
        current_date = last_date + timedelta(days=1)
        while len(future_dates) < 90:  # çº¦3ä¸ªæœˆ
            if current_date not in future_dates:
                future_dates.append(current_date)
            current_date += timedelta(days=1)

        future_dates = pd.to_datetime(future_dates)
        print(f"ğŸ“… é¢„æµ‹æ—¶é—´èŒƒå›´: {future_dates.min()} åˆ° {future_dates.max()}")

        # å‡†å¤‡æœªæ¥ç‰¹å¾
        future_features = self.prepare_future_features(historical_data, future_dates)

        # ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆä¸¥æ ¼ä½¿ç”¨è®­ç»ƒæ—¶çš„scalerï¼‰
        future_features_scaled = self.scaler.transform(future_features)

        # é¢„æµ‹ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
        print("ğŸ”„ æ­£åœ¨è¿›è¡Œå¤šè¾“å‡ºé¢„æµ‹...")
        all_predictions = []
        with tqdm(total=len(future_features_scaled), desc="é¢„æµ‹è¿›åº¦") as pbar:
            for i in range(len(future_features_scaled)):
                pred = self.model.predict(future_features_scaled[i:i + 1])[0]
                all_predictions.append(pred)
                pbar.update(1)

        # å¤„ç†é¢„æµ‹ç»“æœï¼ˆç¡®ä¿ç»´åº¦åŒ¹é…ï¼‰
        predictions = np.vstack(all_predictions)
        print(f"ğŸ” é¢„æµ‹ç»“æœå½¢çŠ¶: {predictions.shape}")

        # ç¡®ä¿é¢„æµ‹ç»“æœç»´åº¦ä¸ç›®æ ‡åˆ—ä¸€è‡´
        if predictions.shape[1] != len(self.target_columns):
            predictions = predictions[:, :len(self.target_columns)]  # æˆªæ–­å¤šä½™åˆ—

        # åˆ›å»ºé¢„æµ‹ç»“æœDataFrame
        predictions_df = pd.DataFrame(predictions, columns=self.target_columns)
        predictions_df['date'] = future_dates

        # ä¿å­˜é¢„æµ‹ç»“æœ
        predictions_df.to_csv('æœªæ¥3ä¸ªæœˆç”µåŠ›è´Ÿè·é¢„æµ‹ç»“æœ.csv', index=False, encoding='utf-8')
        print("âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: æœªæ¥3ä¸ªæœˆç”µåŠ›è´Ÿè·é¢„æµ‹ç»“æœ.csv")

        # å¯è§†åŒ–é¢„æµ‹ç»“æœ
        self.visualize_predictions(predictions_df)

        return predictions_df

    def visualize_predictions(self, predictions_df):
        """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
        print("ğŸ“Š æ­£åœ¨ç”Ÿæˆé¢„æµ‹å¯è§†åŒ–...")

        # åˆ›å»ºå­å›¾ï¼ˆ4ä¸ªè¡Œä¸šÃ—2ä¸ªæŒ‡æ ‡=8ä¸ªå­å›¾ï¼‰
        fig, axes = plt.subplots(4, 2, figsize=(16, 20))
        axes = axes.flatten()

        industry_names = ['å•†ä¸š', 'å¤§å·¥ä¸šç”¨ç”µ', 'æ™®é€šå·¥ä¸š', 'éæ™®å·¥ä¸š']
        metrics = ['max_power', 'min_power']

        for idx, industry in enumerate(industry_names):
            for metric_idx, metric in enumerate(metrics):
                col_name = f'{industry}_{metric}'
                ax = axes[idx * 2 + metric_idx]

                ax.plot(predictions_df['date'], predictions_df[col_name],
                        linewidth=2, color='blue', marker='o', markersize=3)
                ax.set_title(f'{industry} - {metric.replace("_power", "è´Ÿè·")}', fontsize=12)
                ax.set_xlabel('æ—¥æœŸ')
                ax.set_ylabel('è´Ÿè·å€¼')
                ax.grid(alpha=0.3)
                ax.tick_params(axis='x', rotation=45)

        plt.tight_layout()
        plt.savefig('æœªæ¥3ä¸ªæœˆç”µåŠ›è´Ÿè·é¢„æµ‹å¯è§†åŒ–.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… é¢„æµ‹å¯è§†åŒ–å·²ä¿å­˜åˆ°: æœªæ¥3ä¸ªæœˆç”µåŠ›è´Ÿè·é¢„æµ‹å¯è§†åŒ–.png")

    def analyze_model_performance(self, historical_data):
        """åˆ†ææ¨¡å‹åœ¨å†å²æ•°æ®ä¸Šçš„æ€§èƒ½ï¼ˆä¿®å¤ç‰¹å¾å¯¹é½é—®é¢˜ï¼‰"""
        print("\nğŸ“Š æ­£åœ¨åˆ†ææ¨¡å‹é¢„æµ‹æ•ˆæœ...")

        historical_data = historical_data.copy()
        historical_data['date'] = pd.to_datetime(historical_data['date'])
        historical_data = historical_data.sort_values('date')

        # é€‰æ‹©æœ€è¿‘90å¤©ä½œä¸ºéªŒè¯æœŸ
        validation_start = historical_data['date'].max() - timedelta(days=90)
        validation_data = historical_data[historical_data['date'] >= validation_start]

        if len(validation_data) < 30:
            print("âš ï¸ éªŒè¯æ•°æ®ä¸è¶³ï¼Œè·³è¿‡æ¨¡å‹æ€§èƒ½åˆ†æ")
            return

        print(f"ğŸ” ä½¿ç”¨éªŒè¯æœŸ: {validation_data['date'].min()} åˆ° {validation_data['date'].max()}")

        # å‡†å¤‡éªŒè¯é›†ç‰¹å¾
        feature_data = self.prepare_future_features(
            historical_data[historical_data['date'] < validation_start],
            validation_data['date']
        )

        # ä¸¥æ ¼å¯¹é½ç‰¹å¾ï¼ˆå…³é”®ä¿®å¤ï¼‰
        X_val = feature_data.reindex(columns=self.feature_columns, fill_value=0)
        X_val_scaled = self.scaler.transform(X_val)

        # é¢„æµ‹éªŒè¯é›†
        print("ğŸ”„ éªŒè¯é›†é¢„æµ‹ä¸­...")
        y_pred = self.model.predict(X_val_scaled)
        y_true = validation_data[self.target_columns].values

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        avg_mae = np.mean([mean_absolute_error(y_true[:, i], y_pred[:, i]) for i in range(len(self.target_columns))])
        avg_r2 = np.mean([r2_score(y_true[:, i], y_pred[:, i]) for i in range(len(self.target_columns))])

        print(f"ğŸ“ˆ éªŒè¯é›†æ€§èƒ½:")
        print(f"   - å¹³å‡MAE: {avg_mae:.4f}")
        print(f"   - å¹³å‡RÂ²: {avg_r2:.4f}")

        # å¯è§†åŒ–éªŒè¯ç»“æœ
        self.visualize_validation(y_true, y_pred, validation_data['date'])

    def visualize_validation(self, y_true, y_pred, dates):
        """å¯è§†åŒ–éªŒè¯ç»“æœ"""
        fig, axes = plt.subplots(4, 2, figsize=(16, 20))
        axes = axes.flatten()

        for idx, target in enumerate(self.target_columns):
            ax = axes[idx]
            ax.plot(dates, y_true[:, idx], label='å®é™…å€¼', linewidth=2)
            ax.plot(dates, y_pred[:, idx], label='é¢„æµ‹å€¼', linewidth=2, alpha=0.8)
            ax.set_title(f'{target}', fontsize=12)
            ax.set_xlabel('æ—¥æœŸ')
            ax.set_ylabel('è´Ÿè·å€¼')
            ax.legend()
            ax.grid(alpha=0.3)
            ax.tick_params(axis='x', rotation=45)

        plt.tight_layout()
        plt.savefig('æ¨¡å‹éªŒè¯ç»“æœå¯è§†åŒ–.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… éªŒè¯ç»“æœå¯è§†åŒ–å·²ä¿å­˜åˆ°: æ¨¡å‹éªŒè¯ç»“æœå¯è§†åŒ–.png")


def main():
    """ä¸»å‡½æ•°"""
    print("=================================================================================")
    print("GradientBoostingæ¨¡å‹ - æœªæ¥3ä¸ªæœˆè´Ÿè·é¢„æµ‹ç³»ç»Ÿ")
    print("=================================================================================")

    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = GradientBoostingPredictor()

    # åŠ è½½å†å²æ•°æ®
    historical_data = predictor.load_historical_data()

    # åˆ†ææ¨¡å‹æ€§èƒ½ï¼ˆéªŒè¯ï¼‰
    predictor.analyze_model_performance(historical_data)

    # é¢„æµ‹æœªæ¥3ä¸ªæœˆ
    predictions = predictor.predict_future_3months(historical_data)

    print("\nğŸ‰ é¢„æµ‹å®Œæˆï¼")
    print("ğŸ“‹ è¾“å‡ºæ–‡ä»¶:")
    print("   - æœªæ¥3ä¸ªæœˆç”µåŠ›è´Ÿè·é¢„æµ‹ç»“æœ.csv")
    print("   - æœªæ¥3ä¸ªæœˆç”µåŠ›è´Ÿè·é¢„æµ‹å¯è§†åŒ–.png")
    print("   - æ¨¡å‹éªŒè¯ç»“æœå¯è§†åŒ–.png")


if __name__ == "__main__":
    main()