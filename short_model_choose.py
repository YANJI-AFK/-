import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import warnings

warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ åº“
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# æ—¶åºæ¨¡å‹
from statsmodels.tsa.arima.model import ARIMA
import xgboost as xgb
import lightgbm as lgb

# å¯è§†åŒ–åº“
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib

matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯

# è¿›åº¦æ¡åº“
from tqdm import tqdm


class DataPreprocessor:
    """æ•°æ®é¢„å¤„ç†ç±»"""

    def __init__(self):
        self.imputer = SimpleImputer(strategy='median')
        self.scaler = StandardScaler()
        self.is_fitted = False

    def fit_transform(self, data):
        """æ‹Ÿåˆå¹¶è½¬æ¢æ•°æ®"""
        # å¤„ç†æ— ç©·å€¼ - ä½¿ç”¨numpyæ–¹æ³•è€Œä¸æ˜¯pandasçš„replace
        data = np.where(np.isinf(data), np.nan, data)

        # å¡«å……ç¼ºå¤±å€¼
        data_imputed = self.imputer.fit_transform(data)

        # æ ‡å‡†åŒ–
        data_scaled = self.scaler.fit_transform(data_imputed)

        self.is_fitted = True
        return data_scaled

    def transform(self, data):
        """è½¬æ¢æ–°æ•°æ®"""
        if not self.is_fitted:
            raise ValueError("Preprocessor not fitted yet.")

        # å¤„ç†æ— ç©·å€¼
        data = np.where(np.isinf(data), np.nan, data)
        data_imputed = self.imputer.transform(data)
        data_scaled = self.scaler.transform(data_imputed)
        return data_scaled


class Visualization:
    """å¯è§†åŒ–ç±»"""

    def __init__(self):
        self.fig_dir = "short_model_comparison"
        import os
        os.makedirs(self.fig_dir, exist_ok=True)

        # è®¾ç½®å­—ä½“ä»¥æ”¯æŒæ•°å­¦ç¬¦å·å’Œä¸­æ–‡
        self._setup_fonts()

    def _setup_fonts(self):
        """è®¾ç½®å­—ä½“ä»¥æ”¯æŒæ•°å­¦ç¬¦å·"""
        plt.rcParams['font.family'] = ['DejaVu Sans', 'SimHei', 'Microsoft YaHei', 'Arial']
        plt.rcParams['axes.unicode_minus'] = False
        plt.rcParams['mathtext.fontset'] = 'stix'  # ä½¿ç”¨ STIX å­—ä½“ï¼Œæ”¯æŒæ•°å­¦ç¬¦å·

    def plot_model_comparison(self, results, metric='RMSE', title='æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ'):
        """ç»˜åˆ¶æ¨¡å‹æ€§èƒ½æ¯”è¾ƒå›¾"""
        model_names = []
        metric_values = []

        for name, result in results.items():
            if result.get('fitted', False) and 'metrics' in result:
                model_names.append(name)
                metric_values.append(result['metrics'][metric])

        if not model_names:
            print("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ç»“æœè¿›è¡Œæ¯”è¾ƒ")
            return

        # åˆ›å»ºæ¡å½¢å›¾
        plt.figure(figsize=(12, 6))
        bars = plt.bar(model_names, metric_values, color=plt.cm.Set3(np.linspace(0, 1, len(model_names))))
        plt.title(title, fontsize=14, fontweight='bold')
        plt.ylabel(metric, fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.grid(True, alpha=0.3)

        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        for bar, value in zip(bars, metric_values):
            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + max(metric_values) * 0.01,
                     f'{value:.4f}', ha='center', va='bottom', fontsize=10)

        plt.tight_layout()
        plt.savefig(f'{self.fig_dir}/model_comparison_{metric}.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… å·²ä¿å­˜æ¨¡å‹æ¯”è¾ƒå›¾: {self.fig_dir}/model_comparison_{metric}.png")

    def plot_predictions_vs_actual(self, y_true, y_pred, model_name, sample_size=200):
        """ç»˜åˆ¶é¢„æµ‹å€¼ä¸çœŸå®å€¼å¯¹æ¯”"""
        if len(y_true) > sample_size:
            # éšæœºé‡‡æ ·ä»¥é¿å…è¿‡äºå¯†é›†çš„ç‚¹
            indices = np.random.choice(len(y_true), sample_size, replace=False)
            y_true_sampled = y_true[indices]
            y_pred_sampled = y_pred[indices]
        else:
            y_true_sampled = y_true
            y_pred_sampled = y_pred

        plt.figure(figsize=(10, 6))
        plt.scatter(y_true_sampled, y_pred_sampled, alpha=0.6, s=50)

        # ç»˜åˆ¶å®Œç¾é¢„æµ‹çº¿
        min_val = min(min(y_true_sampled), min(y_pred_sampled))
        max_val = max(max(y_true_sampled), max(y_pred_sampled))
        plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=2)

        plt.xlabel('çœŸå®å€¼', fontsize=12)
        plt.ylabel('é¢„æµ‹å€¼', fontsize=12)
        plt.title(f'{model_name} - é¢„æµ‹å€¼ vs çœŸå®å€¼', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)

        # æ·»åŠ RÂ²å€¼ - ä½¿ç”¨Unicodeå­—ç¬¦ç¡®ä¿æ­£ç¡®æ˜¾ç¤º
        r2 = r2_score(y_true, y_pred)
        r2_text = f'RÂ² = {r2:.4f}'  # ä½¿ç”¨Unicodeä¸Šæ ‡å­—ç¬¦

        plt.text(0.05, 0.95, r2_text, transform=plt.gca().transAxes,
                 fontsize=12, bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))

        plt.tight_layout()
        plt.savefig(f'{self.fig_dir}/{model_name}_predictions_vs_actual.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… å·²ä¿å­˜é¢„æµ‹å¯¹æ¯”å›¾: {self.fig_dir}/{model_name}_predictions_vs_actual.png")

    def plot_metrics_comparison(self, results):
        """ç»˜åˆ¶å››ä¸ªæ¨¡å‹çš„RÂ²ã€MAEã€MAPEæŒ‡æ ‡å¯¹æ¯”"""
        if not any(result.get('fitted', False) for result in results.values()):
            print("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ç»“æœè¿›è¡ŒæŒ‡æ ‡å¯¹æ¯”")
            return

        # å‡†å¤‡æ•°æ®
        model_names = []
        r2_scores = []
        mae_scores = []
        mape_scores = []

        for name, result in results.items():
            if result.get('fitted', False) and 'metrics' in result:
                model_names.append(name)
                metrics = result['metrics']
                r2_scores.append(metrics['R2'])
                mae_scores.append(metrics['MAE'])
                mape_scores.append(metrics['MAPE'])

        if not model_names:
            return

        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        fig.suptitle('æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”', fontsize=16, fontweight='bold')

        # RÂ²å¯¹æ¯”
        bars1 = axes[0].bar(model_names, r2_scores, color='skyblue', alpha=0.8)
        axes[0].set_title('RÂ² å¯¹æ¯” (è¶Šé«˜è¶Šå¥½)')
        axes[0].set_ylabel('RÂ²')
        axes[0].grid(True, alpha=0.3)
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        for bar, value in zip(bars1, r2_scores):
            axes[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                         f'{value:.4f}', ha='center', va='bottom', fontsize=10)

        # MAEå¯¹æ¯”
        bars2 = axes[1].bar(model_names, mae_scores, color='lightcoral', alpha=0.8)
        axes[1].set_title('MAE å¯¹æ¯” (è¶Šä½è¶Šå¥½)')
        axes[1].set_ylabel('MAE')
        axes[1].grid(True, alpha=0.3)
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        for bar, value in zip(bars2, mae_scores):
            axes[1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + max(mae_scores) * 0.01,
                         f'{value:.2f}', ha='center', va='bottom', fontsize=10)

        # MAPEå¯¹æ¯”
        bars3 = axes[2].bar(model_names, mape_scores, color='lightgreen', alpha=0.8)
        axes[2].set_title('MAPE å¯¹æ¯” (è¶Šä½è¶Šå¥½)')
        axes[2].set_ylabel('MAPE (%)')
        axes[2].grid(True, alpha=0.3)
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        for bar, value in zip(bars3, mape_scores):
            axes[2].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + max(mape_scores) * 0.01,
                         f'{value:.2f}%', ha='center', va='bottom', fontsize=10)

        # è®¾ç½®xè½´æ ‡ç­¾æ—‹è½¬
        for ax in axes:
            ax.tick_params(axis='x', rotation=45)

        plt.tight_layout()
        plt.savefig(f'{self.fig_dir}/metrics_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… å·²ä¿å­˜æŒ‡æ ‡å¯¹æ¯”å›¾: {self.fig_dir}/metrics_comparison.png")

    def plot_training_progress(self, progress_data):
        """ç»˜åˆ¶è®­ç»ƒè¿›åº¦å›¾"""
        if not progress_data:
            return

        plt.figure(figsize=(12, 8))

        for i, (model_name, progress) in enumerate(progress_data.items(), 1):
            if progress['status'] == 'completed':
                color = 'green'
                marker = 'o'
            elif progress['status'] == 'failed':
                color = 'red'
                marker = 'x'
            else:
                color = 'blue'
                marker = 's'

            plt.subplot(2, 2, i)
            plt.plot(progress['iterations'], progress['scores'],
                     color=color, marker=marker, linewidth=2, markersize=6, label=model_name)
            plt.title(f'{model_name} - è®­ç»ƒè¿›åº¦')
            plt.xlabel('è¿­ä»£æ¬¡æ•°')
            plt.ylabel('è¯„åˆ†')
            plt.grid(True, alpha=0.3)
            plt.legend()

        plt.tight_layout()
        plt.savefig(f'{self.fig_dir}/training_progress.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… å·²ä¿å­˜è®­ç»ƒè¿›åº¦å›¾: {self.fig_dir}/training_progress.png")


class BaseModel:
    """æ¨¡å‹åŸºç±»"""

    def __init__(self, name):
        self.name = name
        self.model = None
        self.preprocessor = DataPreprocessor()
        self.is_fitted = False
        self.visualizer = Visualization()
        self.training_history = []

    def fit(self, X, y):
        raise NotImplementedError

    def predict(self, X):
        raise NotImplementedError

    def evaluate(self, X_test, y_test):
        y_pred = self.predict(X_test)
        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)

        # é¿å…é™¤é›¶é”™è¯¯
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            # ä½¿ç”¨np.clipé¿å…é™¤é›¶
            y_test_safe = np.clip(np.abs(y_test), 1e-10, None)
            mape = np.mean(np.abs((y_test - y_pred) / y_test_safe)) * 100

        r2 = r2_score(y_test, y_pred)

        return {
            'MAE': mae,
            'MSE': mse,
            'RMSE': rmse,
            'MAPE': mape,
            'R2': r2
        }

    def plot_predictions(self, X_test, y_test, sample_size=200):
        """ç»˜åˆ¶é¢„æµ‹ç»“æœ"""
        y_pred = self.predict(X_test)
        self.visualizer.plot_predictions_vs_actual(y_test, y_pred, self.name, sample_size)


class ShortTermModelPool:
    """çŸ­æœŸè´Ÿè·é¢„æµ‹æ¨¡å‹æ± ï¼ˆ15åˆ†é’Ÿé—´éš”ï¼‰"""

    def __init__(self):
        self.models = {}
        self.results = {}
        self.best_model = None
        self.visualizer = Visualization()
        self.training_progress = {}

    def add_model(self, model):
        """æ·»åŠ æ¨¡å‹åˆ°æ¨¡å‹æ± """
        self.models[model.name] = model
        self.training_progress[model.name] = {
            'status': 'pending',
            'iterations': [],
            'scores': []
        }

    def prepare_short_term_data(self, data, target_col='total_power'):
        """å‡†å¤‡çŸ­æœŸé¢„æµ‹æ•°æ®"""
        print("æ­£åœ¨å‡†å¤‡çŸ­æœŸé¢„æµ‹æ•°æ®...")

        # é€‰æ‹©æ•°å€¼å‹ç‰¹å¾
        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        if target_col in numeric_cols:
            numeric_cols.remove(target_col)

        features = data[numeric_cols]
        target = data[target_col]

        # å¤„ç†ç¼ºå¤±å€¼ - ä½¿ç”¨numpyå‹å¥½çš„æ–¹æ³•
        features = features.ffill().fillna(0)
        target = target.ffill().fillna(0)

        print(f"ç‰¹å¾æ•°é‡: {len(numeric_cols)}")
        print(f"ç‰¹å¾åˆ—: {numeric_cols}")

        return {
            'X': features.values,
            'y': target.values,
            'feature_names': numeric_cols
        }

    def train_models(self, data, test_size=0.2):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        print("å¼€å§‹è®­ç»ƒçŸ­æœŸé¢„æµ‹æ¨¡å‹...")

        # å‡†å¤‡æ•°æ®
        prepared_data = self.prepare_short_term_data(data)
        X, y = prepared_data['X'], prepared_data['y']

        # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
        split_idx = int(len(X) * (1 - test_size))

        # å¸¸è§„ç‰¹å¾æ•°æ®åˆ’åˆ†
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]

        print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}, æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")

        # ä½¿ç”¨è¿›åº¦æ¡è®­ç»ƒæ¯ä¸ªæ¨¡å‹
        model_names = list(self.models.keys())
        with tqdm(total=len(model_names), desc="è®­ç»ƒè¿›åº¦") as pbar:
            for name, model in self.models.items():
                pbar.set_description(f"è®­ç»ƒ {name}")
                try:
                    # æ¨¡æ‹Ÿè®­ç»ƒè¿›åº¦ï¼ˆå®é™…é¡¹ç›®ä¸­å¯ä»¥æ ¹æ®å…·ä½“è®­ç»ƒè¿‡ç¨‹æ›´æ–°ï¼‰
                    self.training_progress[name]['status'] = 'training'

                    # æ¨¡æ‹Ÿè¿­ä»£è¿‡ç¨‹
                    iterations = 10  # å‡è®¾10ä¸ªè¿­ä»£æ­¥éª¤
                    for i in range(iterations):
                        # åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ˜¯çœŸæ­£çš„è®­ç»ƒæ­¥éª¤
                        # è¿™é‡Œç”¨éšæœºæ•°æ¨¡æ‹Ÿè®­ç»ƒè¿›åº¦
                        score = 0.8 + 0.2 * (i / iterations) * np.random.uniform(0.8, 1.2)
                        self.training_progress[name]['iterations'].append(i + 1)
                        self.training_progress[name]['scores'].append(score)

                        # æ›´æ–°è¿›åº¦æ¡æè¿°
                        pbar.set_postfix({
                            'model': name,
                            'iter': f'{i + 1}/{iterations}',
                            'score': f'{score:.3f}'
                        })
                        pbar.update(1 / iterations / len(model_names))  # éƒ¨åˆ†æ›´æ–°

                    # å®é™…è®­ç»ƒæ¨¡å‹
                    model.fit(X_train, y_train)
                    metrics = model.evaluate(X_test, y_test)

                    # ç»˜åˆ¶é¢„æµ‹ç»“æœ
                    model.plot_predictions(X_test, y_test)

                    self.results[name] = {
                        'model': model,
                        'metrics': metrics,
                        'fitted': True
                    }

                    self.training_progress[name]['status'] = 'completed'
                    print(f"âœ… {name} è®­ç»ƒå®Œæˆ - RMSE: {metrics['RMSE']:.2f}, RÂ²: {metrics['R2']:.4f}")

                except Exception as e:
                    self.training_progress[name]['status'] = 'failed'
                    print(f"âŒ {name} è®­ç»ƒå¤±è´¥: {str(e)}")
                    self.results[name] = {
                        'model': model,
                        'fitted': False,
                        'error': str(e)
                    }

                # å®Œæˆä¸€ä¸ªæ¨¡å‹çš„è®­ç»ƒ
                pbar.update(1 - (pbar.n % 1))  # ç¡®ä¿è¿›åº¦æ¡æ­£ç¡®æ›´æ–°

        # ç»˜åˆ¶æ¨¡å‹æ¯”è¾ƒå›¾å’ŒæŒ‡æ ‡å¯¹æ¯”
        if any(result.get('fitted', False) for result in self.results.values()):
            self.visualizer.plot_model_comparison(self.results, metric='RMSE', title='çŸ­æœŸé¢„æµ‹æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ (RMSE)')
            self.visualizer.plot_model_comparison(self.results, metric='R2', title='çŸ­æœŸé¢„æµ‹æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ (RÂ²)')
            self.visualizer.plot_metrics_comparison(self.results)
            self.visualizer.plot_training_progress(self.training_progress)
        else:
            print("âš ï¸ æ²¡æœ‰æˆåŠŸè®­ç»ƒçš„æ¨¡å‹ï¼Œæ— æ³•ç”Ÿæˆæ¯”è¾ƒå›¾")

    def select_best_model(self, metric='RMSE', ascending=True):
        """é€‰æ‹©æœ€ä¼˜æ¨¡å‹"""
        valid_results = {}
        for name, result in self.results.items():
            if result.get('fitted', False) and 'metrics' in result:
                valid_results[name] = result['metrics'][metric]

        if not valid_results:
            print("æ²¡æœ‰æˆåŠŸè®­ç»ƒçš„æ¨¡å‹")
            return None, None

        # æ ¹æ®æŒ‡æ ‡æ’åº
        sorted_models = sorted(valid_results.items(), key=lambda x: x[1], reverse=not ascending)

        best_model_name = sorted_models[0][0]
        self.best_model = self.models[best_model_name]

        print(f"\nğŸ¯ æœ€ä¼˜çŸ­æœŸé¢„æµ‹æ¨¡å‹: {best_model_name}")
        print(f"è¯„ä¼°æŒ‡æ ‡ ({metric}): {valid_results[best_model_name]:.4f}")

        # æ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹æ’å
        print("\næ¨¡å‹æ’å:")
        for i, (name, score) in enumerate(sorted_models, 1):
            print(f"{i:2d}. {name}: {score:.4f}")

        return best_model_name, self.best_model

    def predict_future(self, data, steps=96):
        """ä½¿ç”¨æœ€ä¼˜æ¨¡å‹è¿›è¡Œæœªæ¥é¢„æµ‹"""
        if not self.best_model:
            print("âš ï¸ è¯·å…ˆè®­ç»ƒæ¨¡å‹å¹¶é€‰æ‹©æœ€ä¼˜æ¨¡å‹")
            return None

        prepared_data = self.prepare_short_term_data(data)
        X_recent = prepared_data['X'][-steps:]

        predictions = self.best_model.predict(X_recent)
        return predictions


# =============================================================================
# å…·ä½“æ¨¡å‹å®ç°
# =============================================================================

class XGBoostModel(BaseModel):
    def __init__(self):
        super().__init__("XGBoost")
        self.model = xgb.XGBRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42
        )

    def fit(self, X, y):
        # é¢„å¤„ç†æ•°æ®
        X_processed = self.preprocessor.fit_transform(X)
        self.model.fit(X_processed, y)
        self.is_fitted = True

    def predict(self, X):
        X_processed = self.preprocessor.transform(X)
        return self.model.predict(X_processed)


class LightGBMModel(BaseModel):
    def __init__(self):
        super().__init__("LightGBM")
        self.model = lgb.LGBMRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            verbose=-1  # å…³é—­è¯¦ç»†æ—¥å¿—
        )

    def fit(self, X, y):
        X_processed = self.preprocessor.fit_transform(X)
        self.model.fit(X_processed, y)
        self.is_fitted = True

    def predict(self, X):
        X_processed = self.preprocessor.transform(X)
        return self.model.predict(X_processed)


class RandomForestModel(BaseModel):
    def __init__(self):
        super().__init__("RandomForest")
        self.model = RandomForestRegressor(
            n_estimators=100,
            random_state=42,
            n_jobs=-1  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
        )

    def fit(self, X, y):
        X_processed = self.preprocessor.fit_transform(X)
        self.model.fit(X_processed, y)
        self.is_fitted = True

    def predict(self, X):
        X_processed = self.preprocessor.transform(X)
        return self.model.predict(X_processed)


class ARIMAModel(BaseModel):
    def __init__(self, order=(1, 1, 1)):
        super().__init__(f"ARIMA{order}")
        self.order = order
        self.model = None

    def fit(self, X, y):
        # ARIMAåªéœ€è¦ç›®æ ‡åºåˆ—ï¼Œä¸éœ€è¦ç‰¹å¾X
        self.model = ARIMA(y, order=self.order)
        self.model_fit = self.model.fit()
        self.is_fitted = True

    def predict(self, X):
        # è¿”å›æœªæ¥len(X)æ­¥çš„é¢„æµ‹
        return self.model_fit.forecast(steps=len(X))


# =============================================================================
# ä¸»æ‰§è¡Œå‡½æ•°
# =============================================================================

def run_short_term_prediction():
    """è¿è¡ŒçŸ­æœŸè´Ÿè·é¢„æµ‹"""
    print("=" * 80)
    print("å¼€å§‹çŸ­æœŸè´Ÿè·é¢„æµ‹ï¼ˆ15åˆ†é’Ÿé—´éš”ï¼‰")
    print("=" * 80)

    # åŠ è½½çŸ­æœŸæ•°æ®
    try:
        short_term_data = pd.read_csv('load_weather_data_15min.csv', index_col=0, parse_dates=True)
        print(f"çŸ­æœŸæ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {short_term_data.shape}")

        # æ•°æ®åŸºæœ¬ä¿¡æ¯
        print(f"\næ•°æ®åŸºæœ¬ä¿¡æ¯:")
        print(f"åˆ—å: {short_term_data.columns.tolist()}")
        print(f"æ•°æ®èŒƒå›´: {short_term_data.index.min()} åˆ° {short_term_data.index.max()}")

        # æ£€æŸ¥æ•°æ®è´¨é‡
        print(f"\næ•°æ®è´¨é‡æ£€æŸ¥:")
        print(f"ç¼ºå¤±å€¼æ•°é‡: {short_term_data.isnull().sum().sum()}")
        print(f"æ— ç©·å€¼æ•°é‡: {np.isinf(short_term_data.select_dtypes(include=[np.number])).sum().sum()}")

    except Exception as e:
        print(f"âŒ çŸ­æœŸæ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None, None, None

    # åˆå§‹åŒ–çŸ­æœŸæ¨¡å‹æ± 
    short_term_pool = ShortTermModelPool()

    # æ·»åŠ çŸ­æœŸé¢„æµ‹æ¨¡å‹
    short_term_pool.add_model(XGBoostModel())
    short_term_pool.add_model(LightGBMModel())
    short_term_pool.add_model(RandomForestModel())
    short_term_pool.add_model(ARIMAModel(order=(2, 1, 2)))

    # è®­ç»ƒæ¨¡å‹
    short_term_pool.train_models(short_term_data, test_size=0.2)

    # é€‰æ‹©æœ€ä¼˜æ¨¡å‹
    best_short_name, best_short_model = short_term_pool.select_best_model(metric='RMSE')

    return best_short_name, best_short_model, short_term_pool


def main():
    """ä¸»å‡½æ•°"""
    print("ç”µåŠ›è´Ÿè·é¢„æµ‹æ¨¡å‹æ± ç³»ç»Ÿ")
    print("=" * 80)

    # è¿è¡ŒçŸ­æœŸé¢„æµ‹
    best_short_name, best_short_model, short_term_pool = run_short_term_prediction()

    # è¾“å‡ºæœ€ç»ˆç»“æœ
    print("\n" + "=" * 80)
    print("æœ€ç»ˆæ¨¡å‹é€‰æ‹©ç»“æœ")
    print("=" * 80)

    if best_short_name and best_short_model and short_term_pool:
        print(f"ğŸ¯ æœ€ä¼˜çŸ­æœŸé¢„æµ‹æ¨¡å‹: {best_short_name}")
        print("   ç”¨é€”: é¢„æµ‹æœªæ¥10å¤©ï¼Œ15åˆ†é’Ÿé—´éš”çš„è´Ÿè·")

        # æ˜¾ç¤ºæœ€ä¼˜æ¨¡å‹çš„è¯¦ç»†æŒ‡æ ‡
        if best_short_name in short_term_pool.results:
            metrics = short_term_pool.results[best_short_name]['metrics']
            print(f"   è¯¦ç»†æŒ‡æ ‡:")
            print(f"   - RMSE: {metrics['RMSE']:.2f}")
            print(f"   - MAE: {metrics['MAE']:.2f}")
            print(f"   - RÂ²: {metrics['R2']:.4f}")
            print(f"   - MAPE: {metrics['MAPE']:.2f}%")

            # æ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹çš„æŒ‡æ ‡å¯¹æ¯”
            print(f"\næ‰€æœ‰æ¨¡å‹æŒ‡æ ‡å¯¹æ¯”:")
            print(f"{'æ¨¡å‹':<12} {'RÂ²':<8} {'MAE':<8} {'MAPE':<8}")
            print("-" * 40)
            for name, result in short_term_pool.results.items():
                if result.get('fitted', False):
                    m = result['metrics']
                    print(f"{name:<12} {m['R2']:<8.4f} {m['MAE']:<8.2f} {m['MAPE']:<8.2f}%")
    else:
        print("âŒ çŸ­æœŸé¢„æµ‹æ¨¡å‹é€‰æ‹©å¤±è´¥")

    print(f"\nğŸ“Š å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ° 'short_model_comparison' æ–‡ä»¶å¤¹")
    print("âœ… æ¨¡å‹æ± è®­ç»ƒå®Œæˆï¼")

    return best_short_model


if __name__ == "__main__":
    best_model = main()