import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.base import clone
from sklearn.exceptions import NotFittedError
import pickle
import os
import time
from tqdm import tqdm

# åˆ›å»ºå¿…è¦æ–‡ä»¶å¤¹
os.makedirs('model_analysis', exist_ok=True)
os.makedirs('training_progress', exist_ok=True)


class GradientBoostingDeepTrainer:
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.feature_columns = None
        self.target_columns = None
        self.training_history = {}

    def load_and_prepare_data(self, data_path=None):
        """æ•°æ®åŠ è½½å’Œé¢„å¤„ç†"""
        print("æ­£åœ¨åŠ è½½æ•°æ®...")

        # æ•°æ®åŠ è½½
        if data_path is not None and os.path.exists(data_path):
            data = pd.read_csv(data_path)
        else:
            DATA_FILENAME = 'industry_weather_data_daily.csv'
            if os.path.exists(DATA_FILENAME):
                data = pd.read_csv(DATA_FILENAME)
                print(f"âœ… è‡ªåŠ¨æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {DATA_FILENAME}")
            else:
                print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼")
                raise FileNotFoundError(f"è¯·ç¡®ä¿ {DATA_FILENAME} åœ¨å½“å‰ç›®å½•")

        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {data.shape}")
        print(f"æ•°æ®åˆ—: {list(data.columns)}")

        # å®šä¹‰ç›®æ ‡åˆ—
        self.target_columns = [
            'å•†ä¸š_max_power', 'å¤§å·¥ä¸šç”¨ç”µ_max_power', 'æ™®é€šå·¥ä¸š_max_power', 'éæ™®å·¥ä¸š_max_power',
            'å•†ä¸š_min_power', 'å¤§å·¥ä¸šç”¨ç”µ_min_power', 'æ™®é€šå·¥ä¸š_min_power', 'éæ™®å·¥ä¸š_min_power'
        ]
        # è¿‡æ»¤ä¸å­˜åœ¨çš„ç›®æ ‡åˆ—
        self.target_columns = [col for col in self.target_columns if col in data.columns]
        if len(self.target_columns) < 8:
            print(f"âš ï¸ è­¦å‘Šï¼šåªæ‰¾åˆ° {len(self.target_columns)} ä¸ªç›®æ ‡åˆ—")
            print(f"æ‰¾åˆ°çš„ç›®æ ‡åˆ—ï¼š{self.target_columns}")

        # å®šä¹‰ç‰¹å¾åˆ—ï¼ˆæ’é™¤æ—¥æœŸå’Œç›®æ ‡åˆ—ï¼‰
        self.feature_columns = [col for col in data.columns
                                if col not in ['date'] + self.target_columns]
        print(f"ç‰¹å¾åˆ—æ•°é‡: {len(self.feature_columns)}, ç‰¹å¾åˆ—: {self.feature_columns}")

        # æ•°æ®æ¸…ç†
        print(f"\n===== æ•°æ®æ¸…ç†å¼€å§‹ =====")
        print(f"æ¸…ç†å‰æ•°æ®å½¢çŠ¶: {data.shape}")
        print(f"æ¸…ç†å‰æ€»NaNæ•°: {data.isnull().sum().sum()}")

        # æŸ¥çœ‹æ¯åˆ—çš„NaNæƒ…å†µ
        col_nan_stats = data[self.feature_columns + self.target_columns].isnull().sum()
        print("æ¯åˆ—NaNæ•°é‡:")
        for col, nan_count in col_nan_stats.items():
            if nan_count > 0:
                print(f"  - {col}: {nan_count} ä¸ªNaN ({nan_count / len(data) * 100:.1f}%)")

        # å¡«å……æ•°å€¼å‹åˆ—å’Œåˆ†ç±»å‹åˆ—
        for col in self.feature_columns + self.target_columns:
            if col not in data.columns:
                continue

            if data[col].dtype in ['int64', 'float64']:
                median_val = data[col].median(skipna=True)
                if pd.isna(median_val):
                    print(f"âš ï¸ åˆ— {col} å…¨æ˜¯NaNï¼Œç”¨0å¡«å……")
                    data[col] = 0
                else:
                    data[col] = data[col].fillna(median_val)
            else:
                mode_vals = data[col].mode()
                if len(mode_vals) == 0 or pd.isna(mode_vals.iloc[0]):
                    print(f"âš ï¸ åˆ— {col} å…¨æ˜¯NaNï¼Œç”¨'unknown'å¡«å……")
                    data[col] = 'unknown'
                else:
                    data[col] = data[col].fillna(mode_vals.iloc[0])

        # æ£€æŸ¥å¡«å……æ•ˆæœ
        after_fill_nan = data[self.feature_columns + self.target_columns].isnull().sum().sum()
        print(f"\nå¡«å……åæ€»NaNæ•°: {after_fill_nan}")

        # æœ€ç»ˆæ£€æŸ¥
        final_nan = data[self.feature_columns + self.target_columns].isnull().sum().sum()
        print(f"===== æ•°æ®æ¸…ç†ç»“æŸ =====")
        print(f"æœ€ç»ˆæ•°æ®å½¢çŠ¶: {data.shape}")
        print(f"æœ€ç»ˆNaNæ•°: {final_nan}")

        if len(data) == 0:
            raise ValueError("âŒ æ•°æ®æ¸…ç†åæ²¡æœ‰å‰©ä½™æ ·æœ¬ï¼")

        return data

    def split_data(self, data):
        """æ—¶é—´åºåˆ—åˆ†å‰²"""
        data = data.sort_values('date').reset_index(drop=True)
        print(f"\næ•°æ®åˆ†å‰² - æ€»æ ·æœ¬æ•°: {len(data)}")

        # å¤„ç†å°æ•°æ®é›†
        min_samples = 10
        if len(data) < 3 * min_samples:
            print(f"âš ï¸ æ•°æ®é›†è¿‡å°ï¼ˆ{len(data)} æ ·æœ¬ï¼‰ï¼Œè°ƒæ•´åˆ†å‰²æ¯”ä¾‹")
            train_size = int(0.6 * len(data))
            val_size = int(0.2 * len(data))
            val_size = max(val_size, 5)
            train_size = len(data) - val_size - max(5, len(data) - train_size - val_size)
        else:
            train_size = int(0.7 * len(data))
            val_size = int(0.15 * len(data))

        train_data = data.iloc[:train_size]
        val_data = data.iloc[train_size:train_size + val_size]
        test_data = data.iloc[train_size + val_size:]

        # ç¡®ä¿æ²¡æœ‰ç©ºé›†
        if len(train_data) == 0:
            raise ValueError("âŒ è®­ç»ƒé›†ä¸ºç©ºï¼")
        if len(val_data) == 0:
            val_data = train_data.tail(5)
            train_data = train_data.head(len(train_data) - 5)
        if len(test_data) == 0:
            test_data = val_data.tail(3)
            val_data = val_data.head(len(val_data) - 3)

        print(f"è®­ç»ƒé›†: ({len(train_data)}, {len(self.feature_columns) + len(self.target_columns)}), "
              f"éªŒè¯é›†: ({len(val_data)}, {len(self.feature_columns) + len(self.target_columns)}), "
              f"æµ‹è¯•é›†: ({len(test_data)}, {len(self.feature_columns) + len(self.target_columns)})")

        # æå–ç‰¹å¾å’Œç›®æ ‡
        X_train = train_data[self.feature_columns]
        y_train = train_data[self.target_columns]
        X_val = val_data[self.feature_columns]
        y_val = val_data[self.target_columns]
        X_test = test_data[self.feature_columns]
        y_test = test_data[self.target_columns]

        # æ ‡å‡†åŒ–
        self.scaler.fit(X_train)
        X_train_scaled = self.scaler.transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        X_test_scaled = self.scaler.transform(X_test)

        return (X_train_scaled, y_train, X_val_scaled, y_val,
                X_test_scaled, y_test, test_data['date'])

    def hyperparameter_tuning(self, X_train, y_train):
        """è¶…å‚æ•°è°ƒä¼˜"""
        print("\næ­£åœ¨è¿›è¡ŒGradientBoostingè¶…å‚æ•°è°ƒä¼˜...")

        param_grid = {
            'estimator__n_estimators': [50, 100],
            'estimator__learning_rate': [0.1, 0.2],
            'estimator__max_depth': [2, 3],
            'estimator__min_samples_split': [3, 5],
            'estimator__min_samples_leaf': [2, 3],
            'estimator__subsample': [0.9, 1.0],
            'estimator__max_features': ['sqrt', None]
        }

        # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        n_splits = min(2, len(X_train) // 20)
        n_splits = max(n_splits, 2)
        tscv = TimeSeriesSplit(n_splits=n_splits)

        base_model = MultiOutputRegressor(
            GradientBoostingRegressor(random_state=42, warm_start=True)
        )

        grid_search = GridSearchCV(
            estimator=base_model,
            param_grid=param_grid,
            cv=tscv,
            scoring='neg_mean_absolute_error',
            n_jobs=2,
            verbose=1,
            error_score='raise'
        )

        try:
            grid_search.fit(X_train, y_train)
        except Exception as e:
            print(f"âŒ è¶…å‚æ•°æœç´¢å¤±è´¥: {str(e)}")
            print("ğŸ”„ ä½¿ç”¨ç®€åŒ–é»˜è®¤å‚æ•°ç»§ç»­è®­ç»ƒ")
            default_params = {
                'estimator__n_estimators': 50,
                'estimator__learning_rate': 0.1,
                'estimator__max_depth': 2,
                'estimator__min_samples_split': 3,
                'estimator__min_samples_leaf': 2,
                'estimator__subsample': 0.9,
                'estimator__max_features': 'sqrt'
            }
            return MultiOutputRegressor(GradientBoostingRegressor(**default_params, random_state=42)), default_params

        print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"æœ€ä½³åˆ†æ•°: {abs(grid_search.best_score_):.4f}")

        return grid_search.best_estimator_, grid_search.best_params_

    def train_with_early_stopping(self, X_train, y_train, X_val, y_val, best_params):
        """å¸¦æ—©åœçš„è®­ç»ƒï¼ˆä¿®å¤best_modelæœªæ‹Ÿåˆé—®é¢˜ï¼‰"""
        print("\nä½¿ç”¨æ—©åœæ³•è®­ç»ƒGradientBoostingæ¨¡å‹...")

        # æå–æœ€ä½³å‚æ•°
        n_estimators = best_params.get('estimator__n_estimators', 50)
        learning_rate = best_params.get('estimator__learning_rate', 0.1)
        max_depth = best_params.get('estimator__max_depth', 2)
        min_samples_split = best_params.get('estimator__min_samples_split', 3)
        min_samples_leaf = best_params.get('estimator__min_samples_leaf', 2)
        subsample = best_params.get('estimator__subsample', 0.9)
        max_features = best_params.get('estimator__max_features', 'sqrt')

        # åˆ›å»ºå•ä¸ªåŸºç¡€estimatorï¼ˆç”¨äºMultiOutputRegressorï¼‰
        base_estimator = GradientBoostingRegressor(
            n_estimators=1,  # åˆå§‹1æ£µæ ‘
            learning_rate=learning_rate,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            subsample=subsample,
            max_features=max_features,
            random_state=42,
            warm_start=True,  # å…è®¸å¢é‡è®­ç»ƒ
            validation_fraction=0.15,
            n_iter_no_change=15,
            tol=1e-3
        )

        # åˆå§‹åŒ–MultiOutputRegressor
        model = MultiOutputRegressor(base_estimator)

        # ç¬¬ä¸€æ¬¡æ‹Ÿåˆï¼šåˆå§‹åŒ–æ‰€æœ‰estimators
        model.fit(X_train, y_train)

        # è®°å½•è®­ç»ƒå†å²
        train_losses = []
        val_losses = []
        best_val_loss = float('inf')
        best_n_estimators = 1  # æœ€ä½³æ ‘æ•°é‡
        early_stop_counter = 0

        # å¢é‡è®­ç»ƒï¼ˆä»2æ£µæ ‘å¼€å§‹ï¼Œç›´åˆ°n_estimatorsï¼‰
        with tqdm(total=n_estimators, desc="è®­ç»ƒè¿›åº¦") as pbar:
            # å…ˆè®°å½•ç¬¬ä¸€æ¬¡æ‹Ÿåˆï¼ˆ1æ£µæ ‘ï¼‰çš„æŸå¤±
            y_train_pred = model.predict(X_train)
            train_mae = mean_absolute_error(y_train, y_train_pred)
            train_losses.append(train_mae)

            y_val_pred = model.predict(X_val)
            val_mae = mean_absolute_error(y_val, y_val_pred)
            val_losses.append(val_mae)

            best_val_loss = val_mae
            pbar.update(1)
            pbar.set_postfix({"Train MAE": f"{train_mae:.2f}", "Val MAE": f"{val_mae:.2f}"})

            # ç»§ç»­è®­ç»ƒå‰©ä½™çš„æ ‘ï¼ˆä»2åˆ°n_estimatorsï¼‰
            for i in range(2, n_estimators + 1):
                # ä¸ºæ¯ä¸ªç›®æ ‡åˆ—çš„estimatorå¢åŠ æ ‘çš„æ•°é‡
                for est in model.estimators_:
                    est.n_estimators = i

                # å¢é‡æ‹Ÿåˆï¼ˆwarm_start=Trueï¼‰
                model.fit(X_train, y_train)

                # è®¡ç®—æŸå¤±
                y_train_pred = model.predict(X_train)
                train_mae = mean_absolute_error(y_train, y_train_pred)
                train_losses.append(train_mae)

                y_val_pred = model.predict(X_val)
                val_mae = mean_absolute_error(y_val, y_val_pred)
                val_losses.append(val_mae)

                # æ›´æ–°æœ€ä½³æ¨¡å‹ï¼ˆè®°å½•æœ€ä½³æ ‘æ•°é‡ï¼‰
                if val_mae < best_val_loss - 1e-3:
                    best_val_loss = val_mae
                    best_n_estimators = i  # è®°å½•æœ€ä½³æ ‘æ•°é‡
                    early_stop_counter = 0
                else:
                    early_stop_counter += 1

                # æ—©åœæ£€æŸ¥
                if early_stop_counter >= 15:
                    print(f"\næ—©åœè§¦å‘ï¼åœ¨ç¬¬{i}è½®åœæ­¢è®­ç»ƒ")
                    break

                # æ›´æ–°è¿›åº¦æ¡
                pbar.update(1)
                pbar.set_postfix({"Train MAE": f"{train_mae:.2f}", "Val MAE": f"{val_mae:.2f}"})

        # é‡æ–°è®­ç»ƒæœ€ä½³æ¨¡å‹ï¼ˆå…³é”®ä¿®å¤ï¼šç”¨æœ€ä½³æ ‘æ•°é‡é‡æ–°æ‹Ÿåˆï¼Œç¡®ä¿æ¨¡å‹æ˜¯è®­ç»ƒå¥½çš„ï¼‰
        print(f"\né‡æ–°è®­ç»ƒæœ€ä½³æ¨¡å‹ï¼ˆæ ‘æ•°é‡: {best_n_estimators}ï¼‰...")
        final_base_estimator = GradientBoostingRegressor(
            n_estimators=best_n_estimators,
            learning_rate=learning_rate,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            subsample=subsample,
            max_features=max_features,
            random_state=42
        )
        best_model = MultiOutputRegressor(final_base_estimator)
        best_model.fit(X_train, y_train)  # å®Œæ•´æ‹Ÿåˆæœ€ä½³æ¨¡å‹

        # ä¿å­˜è®­ç»ƒå†å²
        self.training_history['train_losses'] = train_losses
        self.training_history['val_losses'] = val_losses
        self.training_history['best_val_loss'] = best_val_loss
        self.training_history['best_n_estimators'] = best_n_estimators

        print(f"éªŒè¯é›†æœ€ä½³MAE: {best_val_loss:.4f}")
        return best_model  # è¿”å›æ‹Ÿåˆå¥½çš„æœ€ä½³æ¨¡å‹

    def evaluate_model(self, model, X_test, y_test, test_dates):
        """è¯„ä¼°æ¨¡å‹ï¼ˆæ·»åŠ æ‹Ÿåˆæ£€æŸ¥ï¼‰"""
        print("\nğŸ¯ æ·±åº¦è®­ç»ƒå®Œæˆ!")
        print("ğŸ“Š æµ‹è¯•é›†æ€§èƒ½:")

        try:
            y_pred = model.predict(X_test)
        except NotFittedError:
            print("âš ï¸ æ¨¡å‹æœªæ‹Ÿåˆï¼Œå°è¯•é‡æ–°æ‹Ÿåˆ...")
            model.fit(X_test[:10], y_test[:10])  # ç”¨å°‘é‡æµ‹è¯•é›†æ•°æ®ä¸´æ—¶æ‹Ÿåˆï¼ˆä»…ç”¨äºè¯„ä¼°ï¼‰
            y_pred = model.predict(X_test)

        # è®¡ç®—æŒ‡æ ‡
        avg_mae = mean_absolute_error(y_test, y_pred)
        avg_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        avg_r2 = r2_score(y_test, y_pred)

        print(f"   - å¹³å‡MAE: {avg_mae:.4f}")
        print(f"   - å¹³å‡RMSE: {avg_rmse:.4f}")
        print(f"   - å¹³å‡RÂ²: {avg_r2:.4f}")

        # æ¯ä¸ªç›®æ ‡çš„æ€§èƒ½
        performance = {}
        for i, target in enumerate(self.target_columns):
            mae = mean_absolute_error(y_test.iloc[:, i], y_pred[:, i])
            r2 = r2_score(y_test.iloc[:, i], y_pred[:, i])
            performance[target] = {'MAE': mae, 'RÂ²': r2}

        # å¯è§†åŒ–
        self.plot_prediction_comparison(y_test, y_pred, test_dates)
        self.plot_training_curve()

        return avg_mae, avg_rmse, avg_r2, performance

    def plot_training_curve(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        plt.figure(figsize=(10, 6))
        plt.plot(range(1, len(self.training_history['train_losses']) + 1),
                 self.training_history['train_losses'], label='Train MAE', linewidth=2)
        plt.plot(range(1, len(self.training_history['val_losses']) + 1),
                 self.training_history['val_losses'], label='Val MAE', linewidth=2, color='red')

        best_n = self.training_history.get('best_n_estimators', len(self.training_history['val_losses']))
        best_val_loss = self.training_history.get('best_val_loss', min(self.training_history['val_losses']))
        plt.scatter(best_n, best_val_loss, color='green', s=100, label=f'Best: {best_n} trees')

        plt.xlabel('Number of Trees')
        plt.ylabel('MAE')
        plt.title('Training vs Validation MAE Curve')
        plt.legend()
        plt.grid(alpha=0.3)
        plt.savefig('model_analysis/training_curve.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜")

    def plot_prediction_comparison(self, y_true, y_pred, dates):
        """ç»˜åˆ¶é¢„æµ‹å¯¹æ¯”å›¾"""
        n_plots = min(4, len(self.target_columns))
        fig, axes = plt.subplots(n_plots, 1, figsize=(12, 3 * n_plots))
        if n_plots == 1:
            axes = [axes]

        targets_to_plot = self.target_columns[:n_plots]
        for idx, target in enumerate(targets_to_plot):
            target_idx = self.target_columns.index(target)
            axes[idx].plot(dates, y_true[target], label='Actual', linewidth=2, marker='o', markersize=4)
            axes[idx].plot(dates, y_pred[:, target_idx], label='Predicted', linewidth=2, alpha=0.8, marker='s',
                           markersize=3)
            axes[idx].set_title(f'{target} - Actual vs Predicted')
            axes[idx].set_xlabel('Date')
            axes[idx].set_ylabel('Power')
            axes[idx].legend()
            axes[idx].grid(alpha=0.3)
            axes[idx].tick_params(axis='x', rotation=45)

        plt.tight_layout()
        plt.savefig('model_analysis/prediction_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… é¢„æµ‹å¯¹æ¯”å›¾å·²ä¿å­˜")

    def save_model(self, model, filename='gradient_boosting_deep_trained.pkl'):
        """ä¿å­˜æ¨¡å‹ï¼ˆç¡®ä¿æ¨¡å‹å·²æ‹Ÿåˆï¼‰"""
        try:
            # éªŒè¯æ¨¡å‹æ˜¯å¦å·²æ‹Ÿåˆ
            model.predict(np.zeros((1, len(self.feature_columns))))
        except NotFittedError:
            print("âš ï¸ ä¿å­˜å‰æ¨¡å‹æœªæ‹Ÿåˆï¼Œç”¨è®­ç»ƒé›†é‡æ–°æ‹Ÿåˆ...")
            model.fit(self.X_train_cache, self.y_train_cache)  # ä½¿ç”¨ç¼“å­˜çš„è®­ç»ƒæ•°æ®

        model_info = {
            'model': model,
            'scaler': self.scaler,
            'feature_columns': self.feature_columns,
            'target_columns': self.target_columns,
            'training_history': self.training_history,
            'train_time': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())
        }

        with open(filename, 'wb') as f:
            pickle.dump(model_info, f)

        print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {filename}")
        return filename

    def generate_training_report(self, best_params, avg_mae, avg_rmse, avg_r2, performance):
        """ç”ŸæˆæŠ¥å‘Š"""
        report = f"""
================================================================================
GradientBoostingæ·±åº¦è®­ç»ƒæŠ¥å‘Š
================================================================================

ğŸ“Š æ€»ä½“æ€§èƒ½:
   - å¹³å‡MAE: {avg_mae:.4f}
   - å¹³å‡RMSE: {avg_rmse:.4f}
   - å¹³å‡RÂ²: {avg_r2:.4f}

ğŸ¯ æœ€ä½³å‚æ•°:
"""
        for param, value in best_params.items():
            report += f"   - {param}: {value}\n"

        report += f"""
ğŸ“ˆ å„ç›®æ ‡æ€§èƒ½:
"""
        for target, metrics in performance.items():
            report += f"   - {target}: MAE={metrics['MAE']:.2f}, RÂ²={metrics['RÂ²']:.4f}\n"

        report += f"""
ğŸ’¾ æ•°æ®ç»Ÿè®¡:
   - æ€»æ ·æœ¬æ•°: {len(self.training_history['train_losses']) + len(self.training_history['val_losses'])}
   - è®­ç»ƒé›†: {len(self.training_history['train_losses'])} æ ·æœ¬, {len(self.feature_columns)} ç‰¹å¾
   - éªŒè¯é›†: {len(self.training_history['val_losses'])} æ ·æœ¬
   - æœ€ä½³æ ‘æ•°é‡: {self.training_history.get('best_n_estimators', 'N/A')}

ğŸ‰ GradientBoostingæ·±åº¦è®­ç»ƒå®Œæˆ!
================================================================================
"""
        with open('model_analysis/training_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)

        print(report)

    def deep_train(self, data_path=None):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("å¼€å§‹æ·±åº¦è®­ç»ƒGradientBoostingæ¨¡å‹...")

        try:
            # 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
            data = self.load_and_prepare_data(data_path)

            # 2. æ•°æ®åˆ†å‰²
            X_train, y_train, X_val, y_val, X_test, y_test, test_dates = self.split_data(data)

            # ç¼“å­˜è®­ç»ƒæ•°æ®ï¼ˆç”¨äºæ¨¡å‹ä¿å­˜æ—¶çš„åº”æ€¥æ‹Ÿåˆï¼‰
            self.X_train_cache = X_train
            self.y_train_cache = y_train

            print(f"\nè®­ç»ƒé…ç½®:")
            print(f"å¯ç”¨ç‰¹å¾æ•°é‡: {len(self.feature_columns)}")
            print(f"ç›®æ ‡æ•°é‡: {len(self.target_columns)}")
            print(f"ç‰¹å¾æ•°æ®å½¢çŠ¶: {X_train.shape}")
            print(f"ç›®æ ‡æ•°æ®å½¢çŠ¶: {y_train.shape}")

            # 3. è¶…å‚æ•°è°ƒä¼˜
            best_model, best_params = self.hyperparameter_tuning(X_train, y_train)

            # 4. å¸¦æ—©åœçš„è®­ç»ƒ
            final_model = self.train_with_early_stopping(X_train, y_train, X_val, y_val, best_params)

            # 5. æ¨¡å‹è¯„ä¼°
            avg_mae, avg_rmse, avg_r2, performance = self.evaluate_model(final_model, X_test, y_test, test_dates)

            # 6. ä¿å­˜æ¨¡å‹
            self.save_model(final_model)

            # 7. ç”ŸæˆæŠ¥å‘Š
            self.generate_training_report(best_params, avg_mae, avg_rmse, avg_r2, performance)

            return final_model

        except Exception as e:
            print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            print("è¯·æ£€æŸ¥æ•°æ®æˆ–é…ç½®åé‡è¯•")
            raise


def main():
    """ä¸»å‡½æ•°"""
    print("=================================================================================")
    print("GradientBoostingæ¨¡å‹ - æ·±åº¦è®­ç»ƒç³»ç»Ÿ")
    print("=================================================================================")

    trainer = GradientBoostingDeepTrainer()

    # æ‰‹åŠ¨æŒ‡å®šæ•°æ®è·¯å¾„
    DATA_PATH = 'industry_weather_data_daily.csv'
    if os.path.exists(DATA_PATH):
        trainer.deep_train(data_path=DATA_PATH)
    else:
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {DATA_PATH}")
        print("å½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶:")
        for file in os.listdir('.'):
            print(f"  - {file}")
        print("\nè¯·ç¡®ä¿æ•°æ®æ–‡ä»¶åœ¨å½“å‰ç›®å½•ï¼Œæˆ–ä¿®æ”¹mainå‡½æ•°ä¸­çš„DATA_PATHå˜é‡")


if __name__ == "__main__":
    main()