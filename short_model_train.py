import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import joblib
from tqdm import tqdm
import os

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']  # ç³»ç»Ÿå·²æœ‰çš„ä¸­æ–‡å­—ä½“åˆ—è¡¨
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜
warnings.filterwarnings('ignore')


class DataPreprocessor:
    """æ•°æ®é¢„å¤„ç†ç±»"""

    def __init__(self):
        self.imputer = SimpleImputer(strategy='median')
        self.scaler = StandardScaler()
        self.is_fitted = False
        self.feature_cols = None

    def fit_transform(self, data, feature_cols):
        """æ‹Ÿåˆå¹¶è½¬æ¢æ•°æ®"""
        self.feature_cols = feature_cols
        data = np.array(data, dtype=float)

        # å¤„ç†æ— ç©·å€¼
        data = np.where(np.isinf(data), np.nan, data)

        # å¡«å……ç¼ºå¤±å€¼
        data_imputed = self.imputer.fit_transform(data)

        # æ ‡å‡†åŒ–
        data_scaled = self.scaler.fit_transform(data_imputed)

        self.is_fitted = True
        return data_scaled

    def transform(self, data):
        """è½¬æ¢æ–°æ•°æ®"""
        if not self.is_fitted:
            raise ValueError("é¢„å¤„ç†æ¨¡å‹å°šæœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨fit_transform")

        data = np.array(data, dtype=float)
        data = np.where(np.isinf(data), np.nan, data)
        data_imputed = self.imputer.transform(data)
        data_scaled = self.scaler.transform(data_imputed)
        return data_scaled

    def save(self, path):
        """ä¿å­˜é¢„å¤„ç†æ¨¡å‹"""
        joblib.dump({
            'imputer': self.imputer,
            'scaler': self.scaler,
            'is_fitted': self.is_fitted,
            'feature_cols': self.feature_cols  # ç¡®ä¿è¿™ä¸€è¡Œå­˜åœ¨
        }, path)

    @classmethod
    def load(cls, path):
        """åŠ è½½é¢„å¤„ç†æ¨¡å‹"""
        obj = cls()
        data = joblib.load(path)
        obj.imputer = data['imputer']
        obj.scaler = data['scaler']
        obj.is_fitted = data['is_fitted']
        obj.feature_cols = data['feature_cols']
        return obj


class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒç±»"""

    def __init__(self):
        self.model = None
        self.preprocessor = DataPreprocessor()
        self.fig_dir = "short_train_predict_figures"
        os.makedirs(self.fig_dir, exist_ok=True)
        self.feature_cols = None
        self.train_history = {
            'iterations': [],
            'train_rmse': [],
            'val_rmse': []
        }

    def prepare_features(self, data):
        """å‡†å¤‡æ—¶åºç‰¹å¾"""
        print("æ­£åœ¨å‡†å¤‡æ—¶åºç‰¹å¾...")

        df = data.copy()

        # ç¡®ä¿ç›®æ ‡åˆ—æ˜¯æ•°å€¼ç±»å‹
        if 'total_power' in df.columns:
            df['total_power'] = pd.to_numeric(df['total_power'], errors='coerce')

        # æå–æ—¶é—´ç‰¹å¾
        df['hour'] = df.index.hour
        df['dayofweek'] = df.index.dayofweek
        df['month'] = df.index.month
        df['dayofyear'] = df.index.dayofyear
        df['weekofyear'] = df.index.isocalendar().week
        df['is_weekend'] = (df.index.dayofweek >= 5).astype(int)

        # æ·»åŠ æ»åç‰¹å¾
        if 'total_power' in df.columns:
            lags = [1, 2, 3, 4, 24, 48, 96]  # 15åˆ†é’Ÿ, 30åˆ†é’Ÿ, 45åˆ†é’Ÿ, 1å°æ—¶, 6å°æ—¶, 12å°æ—¶, 24å°æ—¶
            for lag in tqdm(lags, desc="ç”Ÿæˆæ»åç‰¹å¾"):
                df[f'load_lag_{lag}'] = df['total_power'].shift(lag)

            # æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾
            windows = [4, 24, 96]  # 1å°æ—¶, 6å°æ—¶, 24å°æ—¶
            for window in tqdm(windows, desc="ç”Ÿæˆæ»šåŠ¨ç‰¹å¾"):
                df[f'load_rolling_mean_{window}'] = df['total_power'].rolling(
                    window=window, min_periods=1).mean()
                df[f'load_rolling_std_{window}'] = df['total_power'].rolling(
                    window=window, min_periods=1).std()

        # å¡«å……ç¼ºå¤±å€¼
        df = df.ffill().bfill().fillna(0)

        # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
        for col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

        df = df.fillna(0)
        print(f"ç‰¹å¾å‡†å¤‡å®Œæˆï¼Œæ€»ç‰¹å¾æ•°: {len(df.columns)}")
        return df

    def train_model(self, data):
        """è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹å¹¶å¯è§†åŒ–è®­ç»ƒè¿›åº¦"""
        print("å¼€å§‹è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")

        # å‡†å¤‡ç‰¹å¾
        df_with_features = self.prepare_features(data)

        # é€‰æ‹©ç‰¹å¾å’Œç›®æ ‡å˜é‡
        self.feature_cols = [col for col in df_with_features.columns if col != 'total_power']
        X = df_with_features[self.feature_cols]
        y = df_with_features['total_power']

        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆæœ€å7å¤©ä½œä¸ºéªŒè¯é›†ï¼‰
        split_date = df_with_features.index.max() - timedelta(days=7)
        train_mask = df_with_features.index <= split_date
        val_mask = df_with_features.index > split_date

        X_train, X_val = X[train_mask], X[val_mask]
        y_train, y_val = y[train_mask], y[val_mask]

        print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}, éªŒè¯é›†å¤§å°: {len(X_val)}")

        # é¢„å¤„ç†æ•°æ®
        X_train_processed = self.preprocessor.fit_transform(X_train.values, self.feature_cols)
        X_val_processed = self.preprocessor.transform(X_val.values)

        # å¯è§†åŒ–è®­ç»ƒè¿›åº¦ - é€æ­¥å¢åŠ æ ‘çš„æ•°é‡
        self._train_with_progress(X_train_processed, y_train, X_val_processed, y_val)

        # è¯„ä¼°æœ€ç»ˆæ¨¡å‹
        y_pred = self.model.predict(X_val_processed)
        metrics = self.calculate_metrics(y_val, y_pred)

        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
        print(f"éªŒè¯é›†è¯„ä¼°æŒ‡æ ‡:")
        print(f"  - RÂ²: {metrics['R2']:.4f}")
        print(f"  - RMSE: {metrics['RMSE']:.2f}")
        print(f"  - MAE: {metrics['MAE']:.2f}")
        print(f"  - MAPE: {metrics['MAPE']:.2f}%")

        # ç»˜åˆ¶è®­ç»ƒç›¸å…³å›¾è¡¨
        self.plot_training_progress()
        self.plot_test_predictions(y_val, y_pred, X_val.index)
        self.plot_model_analysis(data)

        # ä¿å­˜æ¨¡å‹å’Œé¢„å¤„ç†å·¥å…·
        self.save_model()

        return metrics

    def _train_with_progress(self, X_train, y_train, X_val, y_val):
        """å¸¦è¿›åº¦å¯è§†åŒ–çš„è®­ç»ƒè¿‡ç¨‹"""
        total_estimators = 150
        self.model = RandomForestRegressor(
            n_estimators=1,  # åˆå§‹åŒ–ä¸º1æ£µæ ‘
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè®­ç»ƒè¿›åº¦
        for i in tqdm(range(total_estimators), desc="è®­ç»ƒéšæœºæ£®æ—"):
            self.model.n_estimators = i + 1
            self.model.fit(X_train, y_train)

            # è®¡ç®—è®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ†æ•°
            y_train_pred = self.model.predict(X_train)
            y_val_pred = self.model.predict(X_val)

            train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
            val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))

            # è®°å½•è®­ç»ƒå†å²
            self.train_history['iterations'].append(i + 1)
            self.train_history['train_rmse'].append(train_rmse)
            self.train_history['val_rmse'].append(val_rmse)

    def calculate_metrics(self, y_true, y_pred):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        mae = mean_absolute_error(y_true, y_pred)
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)

        # é¿å…é™¤é›¶é”™è¯¯
        y_true_safe = np.clip(np.abs(y_true), 1e-10, None)
        mape = np.mean(np.abs((y_true - y_pred) / y_true_safe)) * 100

        r2 = r2_score(y_true, y_pred)

        return {
            'MAE': mae,
            'MSE': mse,
            'RMSE': rmse,
            'MAPE': mape,
            'R2': r2
        }

    def plot_training_progress(self):
        """ç»˜åˆ¶è®­ç»ƒè¿›åº¦æ›²çº¿"""
        plt.figure(figsize=(12, 6))
        plt.plot(self.train_history['iterations'], self.train_history['train_rmse'],
                 label='è®­ç»ƒé›†RMSE', color='blue', alpha=0.7)
        plt.plot(self.train_history['iterations'], self.train_history['val_rmse'],
                 label='éªŒè¯é›†RMSE', color='red', alpha=0.7)
        plt.title('æ¨¡å‹è®­ç»ƒè¿›åº¦', fontsize=14, fontweight='bold')
        plt.xlabel('å†³ç­–æ ‘æ•°é‡')
        plt.ylabel('RMSE')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # æ ‡è®°æœ€ä½éªŒè¯è¯¯å·®ç‚¹
        min_val_idx = np.argmin(self.train_history['val_rmse'])
        min_val_rmse = self.train_history['val_rmse'][min_val_idx]
        min_val_iter = self.train_history['iterations'][min_val_idx]
        plt.scatter(min_val_iter, min_val_rmse, color='green', s=100, zorder=5)
        plt.annotate(f'æœ€ä½: {min_val_rmse:.2f}',
                     (min_val_iter, min_val_rmse),
                     xytext=(10, 10), textcoords='offset points',
                     bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="green"))

        plt.tight_layout()
        plt.savefig(f'{self.fig_dir}/training_progress.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… å·²ä¿å­˜è®­ç»ƒè¿›åº¦å›¾: {self.fig_dir}/training_progress.png")

    def plot_test_predictions(self, y_true, y_pred, dates):
        """ç»˜åˆ¶éªŒè¯é›†é¢„æµ‹æ•ˆæœ"""
        plt.figure(figsize=(15, 10))

        # ç»˜åˆ¶æ•´ä½“å¯¹æ¯”
        plt.subplot(2, 1, 1)
        plt.plot(dates, y_true, label='çœŸå®å€¼', alpha=0.7, linewidth=1)
        plt.plot(dates, y_pred, label='é¢„æµ‹å€¼', alpha=0.7, linewidth=1)
        plt.title('éªŒè¯é›†è´Ÿè·é¢„æµ‹å¯¹æ¯”', fontsize=14, fontweight='bold')
        plt.ylabel('è´Ÿè·å€¼')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)

        # ç»˜åˆ¶æ•£ç‚¹å›¾
        plt.subplot(2, 1, 2)
        plt.scatter(y_true, y_pred, alpha=0.6, s=20)
        min_val = min(min(y_true), min(y_pred))
        max_val = max(max(y_true), max(y_pred))
        plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)
        plt.xlabel('çœŸå®å€¼')
        plt.ylabel('é¢„æµ‹å€¼')
        plt.title('é¢„æµ‹å€¼ vs çœŸå®å€¼', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)

        # æ·»åŠ RÂ²å€¼
        r2 = r2_score(y_true, y_pred)
        plt.text(0.05, 0.95, f'RÂ² = {r2:.4f}', transform=plt.gca().transAxes,
                 fontsize=12, bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))

        plt.tight_layout()
        plt.savefig(f'{self.fig_dir}/validation_predictions.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… å·²ä¿å­˜éªŒè¯é›†é¢„æµ‹æ•ˆæœå›¾: {self.fig_dir}/validation_predictions.png")

    def plot_model_analysis(self, data):
        """ç»˜åˆ¶æ¨¡å‹åˆ†æå›¾è¡¨"""
        print("\næ­£åœ¨ç»˜åˆ¶æ¨¡å‹åˆ†æå›¾è¡¨...")

        # ç‰¹å¾é‡è¦æ€§åˆ†æ
        if hasattr(self.model, 'feature_importances_'):
            feature_importance = pd.DataFrame({
                'feature': self.feature_cols,
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=False).head(15)

            plt.figure(figsize=(12, 8))
            sns.barplot(x='importance', y='feature', data=feature_importance, palette='viridis')
            plt.title('éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§ (Top 15)', fontsize=14, fontweight='bold')
            plt.xlabel('ç‰¹å¾é‡è¦æ€§')
            plt.ylabel('ç‰¹å¾åç§°')
            plt.tight_layout()
            plt.savefig(f'{self.fig_dir}/feature_importance.png', dpi=300, bbox_inches='tight')
            plt.close()
            print(f"âœ… å·²ä¿å­˜ç‰¹å¾é‡è¦æ€§å›¾: {self.fig_dir}/feature_importance.png")

        # è´Ÿè·æ—¶é—´åºåˆ—åˆ†æ
        plt.figure(figsize=(15, 10))

        # åŸå§‹è´Ÿè·æ•°æ®
        plt.subplot(2, 2, 1)
        plt.plot(data.index, data['total_power'], linewidth=0.5, alpha=0.7)
        plt.title('å†å²è´Ÿè·æ•°æ®', fontsize=12)
        plt.ylabel('è´Ÿè·å€¼')
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)

        # æ—¥è´Ÿè·æ¨¡å¼
        plt.subplot(2, 2, 2)
        daily_pattern = data.groupby(data.index.hour)['total_power'].mean()
        plt.plot(daily_pattern.index, daily_pattern.values, marker='o', color='C1')
        plt.title('å…¸å‹æ—¥è´Ÿè·æ›²çº¿', fontsize=12)
        plt.xlabel('å°æ—¶')
        plt.ylabel('å¹³å‡è´Ÿè·')
        plt.grid(True, alpha=0.3)

        # å‘¨è´Ÿè·æ¨¡å¼
        plt.subplot(2, 2, 3)
        weekly_pattern = data.groupby(data.index.dayofweek)['total_power'].mean()
        days = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥']
        sns.barplot(x=list(range(7)), y=weekly_pattern.values, palette='viridis')
        plt.title('å‘¨è´Ÿè·æ¨¡å¼', fontsize=12)
        plt.xlabel('æ˜ŸæœŸ')
        plt.ylabel('å¹³å‡è´Ÿè·')
        plt.xticks(range(7), days)
        plt.grid(True, alpha=0.3)

        # æœˆè´Ÿè·æ¨¡å¼
        plt.subplot(2, 2, 4)
        monthly_pattern = data.groupby(data.index.month)['total_power'].mean()
        sns.barplot(x=list(range(1, 13)), y=monthly_pattern.values, palette='viridis')
        plt.title('æœˆè´Ÿè·æ¨¡å¼', fontsize=12)
        plt.xlabel('æœˆä»½')
        plt.ylabel('å¹³å‡è´Ÿè·')
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(f'{self.fig_dir}/load_pattern_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… å·²ä¿å­˜è´Ÿè·æ¨¡å¼åˆ†æå›¾: {self.fig_dir}/load_pattern_analysis.png")

    def save_model(self):
        """ä¿å­˜æ¨¡å‹å’Œé¢„å¤„ç†å·¥å…·"""
        joblib.dump(self.model, 'random_forest_model.pkl')
        self.preprocessor.save('data_preprocessor.pkl')
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: random_forest_model.pkl")
        print(f"âœ… é¢„å¤„ç†å·¥å…·å·²ä¿å­˜åˆ°: data_preprocessor.pkl")


def main():
    """ä¸»å‡½æ•°ï¼šè®­ç»ƒéšæœºæ£®æ—æ¨¡å‹"""
    print("=" * 80)
    print("RandomForest - è´Ÿè·é¢„æµ‹æ¨¡å‹è®­ç»ƒ")
    print("=" * 80)

    try:
        # åŠ è½½æ•°æ®
        print("æ­£åœ¨åŠ è½½æ•°æ®...")
        data = pd.read_csv('load_weather_data_15min.csv', index_col=0, parse_dates=True)
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {data.shape}")

        # æ£€æŸ¥ç›®æ ‡åˆ—æ˜¯å¦å­˜åœ¨
        if 'total_power' not in data.columns:
            print("âŒ æ•°æ®ä¸­æœªæ‰¾åˆ° 'total_power' åˆ—")
            return

        # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
        print(f"\næ•°æ®åŸºæœ¬ä¿¡æ¯:")
        print(f"æ—¶é—´èŒƒå›´: {data.index.min()} åˆ° {data.index.max()}")
        print(f"æ•°æ®é¢‘ç‡: {pd.infer_freq(data.index)}")
        print(f"æ€»è®°å½•æ•°: {len(data)}")
        print(f"ç‰¹å¾æ•°é‡: {len(data.columns)}")

        # åˆå§‹åŒ–è®­ç»ƒå™¨
        trainer = ModelTrainer()

        # è®­ç»ƒæ¨¡å‹
        metrics = trainer.train_model(data)

        # è¾“å‡ºæ€»ç»“
        print("\n" + "=" * 80)
        print("æ¨¡å‹è®­ç»ƒå®Œæˆæ€»ç»“")
        print("=" * 80)
        print(f"ğŸ“Š æ¨¡å‹æ€§èƒ½:")
        print(f"   - RÂ²: {metrics['R2']:.4f}")
        print(f"   - RMSE: {metrics['RMSE']:.2f}")
        print(f"   - MAE: {metrics['MAE']:.2f}")
        print(f"   - MAPE: {metrics['MAPE']:.2f}%")

        print(f"\nğŸ“ˆ å¯è§†åŒ–æ–‡ä»¶ä¿å­˜ç›®å½•: {trainer.fig_dir}")

        return metrics

    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()